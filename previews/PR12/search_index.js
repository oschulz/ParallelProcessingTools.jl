var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Modules","page":"API","title":"Modules","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Order = [:module]","category":"page"},{"location":"api/#Types-and-constants","page":"API","title":"Types and constants","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Order = [:type, :constant]","category":"page"},{"location":"api/#Functions-and-macros","page":"API","title":"Functions and macros","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Order = [:macro, :function]","category":"page"},{"location":"api/#Documentation","page":"API","title":"Documentation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [ParallelProcessingTools]\nOrder = [:module, :type, :constant, :macro, :function]","category":"page"},{"location":"api/#ParallelProcessingTools.AbstractThreadLocal","page":"API","title":"ParallelProcessingTools.AbstractThreadLocal","text":"abstract type AbstractThreadLocal{T} end\n\nAbstract type for thread-local values of type T.\n\nThe value for the current thread is accessed via getindex(::AbstractThreadLocal) and `setindex(::AbstractThreadLocal, x).\n\nTo access both regular and thread-local values in a unified manner, use the function getlocalvalue.\n\nTo get the all values across all threads, use the function getallvalues.\n\nDefault implementation is ThreadLocal.\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelProcessingTools.AddProcsMode","page":"API","title":"ParallelProcessingTools.AddProcsMode","text":"abstract type ParallelProcessingTools.AddProcsMode\n\nAbstract supertype for worker process addition modes.\n\nSubtypes must implement:\n\nParallelProcessingTools.addworkers(mode::SomeAddProcsMode)\n\nand may want to specialize:\n\nParallelProcessingTools.worker_init_code(mode::SomeAddProcsMode)\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelProcessingTools.ElasticAddProcsMode","page":"API","title":"ParallelProcessingTools.ElasticAddProcsMode","text":"abstract type ParallelProcessingTools.ElasticAddProcsMode <: ParallelProcessingTools.AddProcsMode\n\nAbstract supertype for worker process addition modes that use the elastic cluster manager.\n\nSubtypes must implement:\n\nParallelProcessingTools.worker_start_command(mode::SomeElasticAddProcsMode, manager::ClusterManagers.ElasticManager)\nParallelProcessingTools.start_elastic_workers(mode::SomeElasticAddProcsMode, manager::ClusterManagers.ElasticManager)\n\nand may want to specialize:\n\nParallelProcessingTools.elastic_addprocs_timeout(mode::SomeElasticAddProcsMode)\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelProcessingTools.ExternalProcesses","page":"API","title":"ParallelProcessingTools.ExternalProcesses","text":"ParallelProcessingTools.ExternalProcesses(;\n    nprocs::Integer = ...\n)\n\nAdd worker processes by starting them externally.\n\nWill log (via @info) a worker start command and then wait for the workers to connect. The user is responsible for starting the specified number of workers externally using that start command.\n\nExample:\n\nmode = ExternalProcesses(nprocs = 4)\naddworkers(mode)\n\nThe user now has to start 4 Julia worker processes externally using the logged start command. This start command can also be retrieved via worker_start_command(mode).\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelProcessingTools.LocalProcesses","page":"API","title":"ParallelProcessingTools.LocalProcesses","text":"LocalProcesses(;\n    nprocs::Integer = 1\n)\n\nMode to add nprocs worker processes on the current host.\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelProcessingTools.SlurmRun","page":"API","title":"ParallelProcessingTools.SlurmRun","text":"SlurmRun(;\n    slurm_flags::Cmd = {defaults}\n    julia_flags::Cmd = {defaults}\n    dir = pwd()\n    user_start::Bool = false\n    timeout::Real = 60\n)\n\nMode to add worker processes via SLURM srun.\n\nsrun and Julia worker julia command line flags are inferred from SLURM environment variables (e.g. when inside of an salloc or batch job), as well as slurm_flags and julia_flags.\n\nWorkers are started with current directory set to dir.\n\nExample:\n\nmode = SlurmRun(slurm_flags = `--ntasks=4 --cpus-per-task=8 --mem-per-cpu=8G`)\naddworkers(mode)\n\nIf user_start is true, then the SLURM srun-command will not be run automatically, instead it will be logged via @info and the user is responsible for running it. This srun-command can also be retrieved via worker_start_command(mode).\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelProcessingTools.ThreadLocal","page":"API","title":"ParallelProcessingTools.ThreadLocal","text":"ThreadLocal{T} <: AbstractThreadLocal{T}\n\nRepresents a thread-local value. See AbstractThreadLocal for the API.\n\nConstructors:\n\nThreadLocal{T}() where {T}\nThreadLocal(value::T) where {T}\nThreadLocal{T}(f::Base.Callable) where {T}\n\nExamples:\n\ntlvalue = ThreadLocal(0)\n@onthreads allthreads() tlvalue[] = Base.Threads.threadid()\ngetallvalues(tlvalue) == allthreads()\n\nrand_value_on_each_thread = ThreadLocal{Float64}(rand)\nall(x -> 0 < x < 1, getallvalues(rand_value_on_each_thread))\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelProcessingTools.@always_everywhere-Tuple{Any}","page":"API","title":"ParallelProcessingTools.@always_everywhere","text":"always_everywhere(expr)\n\nRuns expr on all current Julia processes, but also all future Julia processes added via addworkers).\n\nSimilar to Distributed.everywhere, but also stores expr so that addworkers can execute it automatically on new worker processes.\n\n\n\n\n\n","category":"macro"},{"location":"api/#ParallelProcessingTools.@critical-Tuple{Any}","page":"API","title":"ParallelProcessingTools.@critical","text":"@critical expr\n\nMark code in expr as a critical section. Code in critical sections will never be executed in parallel (via multithreading) to any other critical section.\n\n@critical is very useful to mark non-threadsafe code.\n\nExample:\n\n@onthreads allthreads() begin\n    @critical @info Base.Threads.threadid()\nend\n\nWithout `@critical`, the above will typically crash Julia.\n\n\n\n\n\n","category":"macro"},{"location":"api/#ParallelProcessingTools.@mt_out_of_order-Tuple{Any}","page":"API","title":"ParallelProcessingTools.@mt_out_of_order","text":"@mt_out_of_order begin expr... end\n\nRuns all top-level expressions in begin expr... end on parallel multi-threaded tasks.\n\nExample:\n\n``` @mtoutof_order begin     a = foo()     bar()     c = baz() end\n\nwill run a = foo(), bar() and c = baz() in parallel and in arbitrary order, results of assignments will appear in the outside scope.\n\n\n\n\n\n","category":"macro"},{"location":"api/#ParallelProcessingTools.@onprocs-Tuple{Any, Any}","page":"API","title":"ParallelProcessingTools.@onprocs","text":"@onprocs procsel expr\n\nExecutes expr in parallel on all processes in procsel. Waits until all processes are done. Returns all results as a vector (or as a single scalar value, if procsel itself is a scalar).\n\nExample:\n\nusing Distributed\naddprocs(2)\nworkers() == @onprocs workers() myid()\n\n\n\n\n\n","category":"macro"},{"location":"api/#ParallelProcessingTools.@onthreads-Tuple{Any, Any}","page":"API","title":"ParallelProcessingTools.@onthreads","text":"@onthreads threadsel expr\n\nExecute code in expr in parallel on the threads in threadsel.\n\nthreadsel should be a single thread-ID or a range (or array) of thread-ids. If threadsel == Base.Threads.threadid(), expr is run on the current tread with only minimal overhead.\n\nExample 1:\n\ntlsum = ThreadLocal(0.0)\ndata = rand(100)\n@onthreads allthreads() begin\n    tlsum[] = sum(workpart(data, allthreads(), Base.Threads.threadid()))\nend\nsum(getallvalues(tlsum)) â‰ˆ sum(data)\n\nExample 2:\n\n# Assuming 4 threads:\ntl = ThreadLocal(42)\nthreadsel = 2:3\n@onthreads threadsel begin\n    tl[] = Base.Threads.threadid()\nend\ngetallvalues(tl)[threadsel] == [2, 3]\ngetallvalues(tl)[[1,4]] == [42, 42]\n\n\n\n\n\n","category":"macro"},{"location":"api/#ParallelProcessingTools.@userfriendly_exceptions-Tuple{Any}","page":"API","title":"ParallelProcessingTools.@userfriendly_exceptions","text":"@userfriendly_exceptions expr\n\nTransforms exceptions originating from expr into more user-friendly ones.\n\nIf multiple exceptions originate from parallel code in expr, only one is rethrown, and TaskFailedExceptions and RemoteExceptions are replaced by the original exceptions that caused them.\n\nSee [original_exception] and onlyfirst_exception.\n\n\n\n\n\n","category":"macro"},{"location":"api/#ParallelProcessingTools.addworkers","page":"API","title":"ParallelProcessingTools.addworkers","text":"addworkers(mode::ParallelProcessingTools.AddProcsMode)\n\nAdd Julia worker processes for LEGEND data processing.\n\nBy default ensures that all workers processes use the same Julia project environment as the current process (requires that file systems paths are consistenst across compute hosts).\n\nUse @always_everywhere to run initialization code on all current processes and all future processes added via addworkers:\n\nusing Distributed, ParallelProcessingTools\n\n@always_everywhere begin\n    using SomePackage\n    import SomeOtherPackage\n\n    get_global_value() = 42\nend\n\n# ... some code ...\n\naddworkers(LocalProcesses(nprocs = 4))\n\n# `get_global_value` is available even though workers were added later:\nremotecall_fetch(get_global_value, last(workers()))\n\nSee also worker_resources().\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelProcessingTools.allthreads-Tuple{}","page":"API","title":"ParallelProcessingTools.allthreads","text":"allthreads()\n\nConvencience function, returns an equivalent of 1:Base.Threads.nthreads().\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelProcessingTools.create_files-Tuple{Any, Vararg{AbstractString}}","page":"API","title":"ParallelProcessingTools.create_files","text":"function create_files(\n    f_write, filenames::AbstractString...;\n    overwrite::Bool = true,\n    use_cache::Bool = false, cache_dir::AbstractString = tempdir(),\n    create_dirs::Bool = true, delete_tmp_onerror::Bool=true,\n    verbose::Bool = true\n)\n\nCreates filenames in an atomic fashion via a user-provided function f_write. Returns nothing.\n\nUsing temporary filenames, calls f_write(temporary_filenames...). If f_write doesn't throw an exception, the files temporary_filenames are renamed to filenames. If f_write throws an exception, the temporary files are either deleted (if delete_tmp_onerror is true) or left in place (e.g. for debugging purposes).\n\nIf create_dirs is true, the temporary_filenames are created in cache_dir and then atomically moved to filenames, otherwise, they are created next to filenames (in the same directories).\n\nIf create_dirs is true, directories are created if necessary.\n\nIf all of filenames already exist and overwrite is false, takes no action (or, on case the files are created by other code running in parallel, while f_write is running, does not replace them).\n\nIf verbose is true, uses log-level Logging.Info to log file creation, otherwise Logging.Debug.\n\nThrows an error if only some of the files exist and overwrite is false.\n\nReturns nothing.\n\nExample:\n\ncreate_files(\"foo.txt\", \"bar.txt\", use_cache = true) do foo, bar\n    write(foo, \"Hello\")\n    write(bar, \"World\")\nend\n\nSet ENV[\"JULIA_DEBUG\"] = \"ParallelProcessingTools\" to see a log of all intermediate steps.\n\nOn Linux you can set use_cache = true and cache_dir = \"/dev/shm\" to use the default Linux RAM disk as an intermediate directory.\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelProcessingTools.default_elastic_manager","page":"API","title":"ParallelProcessingTools.default_elastic_manager","text":"ParallelProcessingTools.default_elastic_manager()\nParallelProcessingTools.default_elastic_manager(manager::ClusterManagers.ElasticManager)\n\nGet or set the default elastic cluster manager.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelProcessingTools.elastic_addprocs_timeout","page":"API","title":"ParallelProcessingTools.elastic_addprocs_timeout","text":"ParallelProcessingTools.elastic_addprocs_timeout(mode::ElasticAddProcsMode)\n\nGet the timeout in seconds for waiting for worker processes to connect.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelProcessingTools.elastic_localworker_startcmd-Tuple{Distributed.ClusterManager}","page":"API","title":"ParallelProcessingTools.elastic_localworker_startcmd","text":"ParallelProcessingTools.elastic_localworker_startcmd(\n    manager::Distributed.ClusterManager;\n    julia_cmd::Cmd = _default_julia_cmd(),\n    julia_flags::Cmd = _default_julia_flags(),\n    julia_project::AbstractString = _default_julia_project()\n)::Cmd\n\nReturn the system command required to start a Julia worker process, that will connect to manager, on the current host.\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelProcessingTools.getallvalues","page":"API","title":"ParallelProcessingTools.getallvalues","text":"getallvalues(v::AbstractThreadLocal{T})::AbstractVector{T}\n\nAccess the all values (one for each thread) of a thread-local value as a vector. Can only be called in single-threaded code sections.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelProcessingTools.getlocalvalue","page":"API","title":"ParallelProcessingTools.getlocalvalue","text":"getlocalvalue(x::Any) = x\ngetlocalvalue(x::ThreadLocal) = x[]\n\nAccess plain values and thread-local values in a unified fashion.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelProcessingTools.onlyfirst_exception","page":"API","title":"ParallelProcessingTools.onlyfirst_exception","text":"ParallelProcessingTools.onlyfirst_exception(err)\n\nReplaces CompositeExceptions with their first exception.\n\nAlso employs original_exception if simplify is true.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelProcessingTools.original_exception","page":"API","title":"ParallelProcessingTools.original_exception","text":"ParallelProcessingTools.original_exception(err)\n\nReplaces TaskFailedExceptions and RemoteExceptions with the underlying exception that originated within the task or on the remote process.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelProcessingTools.pinthreads_auto-Tuple{}","page":"API","title":"ParallelProcessingTools.pinthreads_auto","text":"pinthreads_auto()\n\nUse default thread-pinning strategy for the current Julia process.\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelProcessingTools.pinthreads_distributed-Tuple{AbstractVector{<:Integer}}","page":"API","title":"ParallelProcessingTools.pinthreads_distributed","text":"ParallelProcessingTools.pinthreads_distributed(procs::AbstractVector{<:Integer} = Distrib)\n\nUse default thread-pinning strategy on all Julia processes processes procs.\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelProcessingTools.read_files-Tuple{Any, Vararg{AbstractString}}","page":"API","title":"ParallelProcessingTools.read_files","text":"function read_files(\n    f_read, filenames::AbstractString...;\n    use_cache::Bool = true, cache_dir::AbstractString = tempdir(),\n    create_cachedir::Bool = true, delete_tmp_onerror::Bool=true,\n    verbose::Bool = true\n)\n\nReads filenames in an atomic fashion (i.e. only if all filenames exist) via a user-provided function f_read. The returns value of f_read is passed through.\n\nIf use_cache is true, then the files are first copied to the temporary directory cache_dir under temporary names, and f_read(temporary_filenames...) is called. The temporary files are deleted afterwards.\n\nIf create_cachedir is true, then cache_dir will be created if it doesn't exist yet. If delete_tmp_onerror is true, then temporary files are deleted even if f_write throws an exception.\n\nIf verbose is true, uses log-level Logging.Info to log file reading, otherwise Logging.Debug.\n\nwrite(\"foo.txt\", \"Hello\"); write(\"bar.txt\", \"World\")\n\nread_files(\"foo.txt\", \"bar.txt\", use_cache = true) do foo, bar\n    read(foo, String) * \" \" * read(bar, String)\nend\n\nSet ENV[\"JULIA_DEBUG\"] = \"ParallelProcessingTools\" to see a log of all intermediate steps.\n\nOn Linux you can set use_cache = true and cache_dir = \"/dev/shm\" to use the default Linux RAM disk as an intermediate directory.\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelProcessingTools.shutdown_workers_atexit-Tuple{}","page":"API","title":"ParallelProcessingTools.shutdown_workers_atexit","text":"ParallelProcessingTools.shutdown_workers_atexit()\n\nEnsure worker processes are shut down when Julia exits.\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelProcessingTools.split_basename_ext-Tuple{AbstractString}","page":"API","title":"ParallelProcessingTools.split_basename_ext","text":"ParallelProcessingTools.split_basename_ext(file_basename_with_ext::AbstractString)\n\nSplits a filename (given without its directory path) into a basename without file extension and the file extension. Returns a tuple (basename_noext, ext).\n\nExample:\n\nParallelProcessingTools.split_basename_ext(\"myfile.tar.gz\") == (\"myfile\", \".tar.gz\")\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelProcessingTools.start_elastic_workers","page":"API","title":"ParallelProcessingTools.start_elastic_workers","text":"ParallelProcessingTools.start_elastic_workers(mode::ElasticAddProcsMode, manager::ClusterManagers.ElasticManager)::Int\n\nSpawn worker processes as specified by mode and return the number of expected additional workers.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelProcessingTools.tmp_filename","page":"API","title":"ParallelProcessingTools.tmp_filename","text":"ParallelProcessingTools.tmp_filename(fname::AbstractString)\nParallelProcessingTools.tmp_filename(fname::AbstractString, dir::AbstractString)\n\nReturns a temporary filename, based on fname.\n\nBy default, the temporary filename is in the same directory as fname, otherwise in dir.\n\nDoes not create the temporary file, only returns the filename (including directory path).\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelProcessingTools.worker_init_code","page":"API","title":"ParallelProcessingTools.worker_init_code","text":"ParallelProcessingTools.worker_init_code(::AddProcsMode)::Expr\n\nGet a Julia code expression to run on new worker processes even before running @always_everywhere code on them.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelProcessingTools.worker_resources-Tuple{}","page":"API","title":"ParallelProcessingTools.worker_resources","text":"worker_resources\n\nGet the distributed Julia process resources currently available.\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelProcessingTools.worker_start_command","page":"API","title":"ParallelProcessingTools.worker_start_command","text":"ParallelProcessingTools.worker_start_command(\n    mode::ElasticAddProcsMode,\n    manager::ClusterManagers.ElasticManager = ParallelProcessingTools.default_elastic_manager()\n)::Tuple{Cmd,Integer}\n\nReturn the system command to start worker processes as well as the number of workers to start.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelProcessingTools.workpart","page":"API","title":"ParallelProcessingTools.workpart","text":"workpart(data::AbstractArray, workersel::AbstractVector{W}, current_worker::W) where {W}\n\nGet the part of data that the execution unit current_worker is responsible for. Implies a partition of data across the workers listed in workersel.\n\nFor generic data arrays, workpart will return a view. If data is a Range (e.g. indices to be processed), a sub-range will be returned.\n\nType W will typically be Int and workersel will usually be a range/array of thread/process IDs.\n\nNote: workersel is required to be sorted in ascending order and to contain no duplicate entries.\n\nExamples:\n\nusing Distributed, Base.Threads\nA = rand(100)\n# ...\nsub_A = workpart(A, workers(), myid())\n# ...\nidxs = workpart(eachindex(sub_A), allthreads(), threadid())\nfor i in idxs\n    # ...\nend\n\n\n\n\n\n","category":"function"},{"location":"LICENSE/#LICENSE","page":"LICENSE","title":"LICENSE","text":"","category":"section"},{"location":"LICENSE/","page":"LICENSE","title":"LICENSE","text":"using Markdown\nMarkdown.parse_file(joinpath(@__DIR__, \"..\", \"..\", \"LICENSE.md\"))","category":"page"},{"location":"#ParallelProcessingTools.jl","page":"Home","title":"ParallelProcessingTools.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This Julia package provides some tools to ease multithreaded and distributed programming.","category":"page"},{"location":"#Compute-cluster-management","page":"Home","title":"Compute cluster management","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ParallelProcessingTools helps spin-up Julia compute clusters. It currently has support for clusters on localhost and on SLURM (uses ClusterManagers.ElasticManager internally).","category":"page"},{"location":"","page":"Home","title":"Home","text":"On SLURM, addworkers will automatically try to perform a sensible thread-pinning (using the ThreadPinning package internally).","category":"page"},{"location":"","page":"Home","title":"Home","text":"using ParallelProcessingTools, Distributed\n\n@always_everywhere begin\n    using Distributions\nend\n\nmode = ParallelProcessingTools.SlurmRun(slurm_flags = `--ntasks=4 --cpus-per-task=8 --mem-per-cpu=8G`)\n#ParallelProcessingTools.worker_start_command(mode)\n\n# Add some workers:\naddworkers(mode)\n\n# List resources:\nParallelProcessingTools.worker_resources()\n\n# Confirm that Distributions is loaded on workers:\nworker = last(workers())\n@fetchfrom worker Normal()\n\n# Add some more workers:\naddworkers(mode)\nTable(ParallelProcessingTools.worker_resources())\n\n# Add even more workers:\naddworkers(mode)\nTable(ParallelProcessingTools.worker_resources())","category":"page"},{"location":"","page":"Home","title":"Home","text":"And we can do SLURM batch scripts like this (e.g. \"batchtest.jl\"):","category":"page"},{"location":"","page":"Home","title":"Home","text":"#!/usr/bin/env -S julia --project=@SOME_JULIA_ENVIRONMENT --threads=8\n#SBATCH --ntasks=4 --cpus-per-task=8 --mem-per-cpu=8G\n\nusing ParallelProcessingTools, Distributed\n\n@always_everywhere begin\n    using ParallelProcessingTools\nend\n\naddworkers(SlurmRun())\nresources = ParallelProcessingTools.worker_resources()\nshow(stdout, MIME\"text/plain\"(), ParallelProcessingTools.worker_resources())","category":"page"},{"location":"","page":"Home","title":"Home","text":"This should run with a simple","category":"page"},{"location":"","page":"Home","title":"Home","text":"sbatch -o out.txt batchtest.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"and \"out.txt\" should then contain a list of the worker resources.","category":"page"}]
}
